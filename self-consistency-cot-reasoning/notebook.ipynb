{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e56f4f",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Reasoning with Self-Consistency\n",
    "Below is a small demo for illustrating the power of self-consistency in chain-of-thought reasoning. I compare the greedy decoding technique (`temperature` = 0) with the self-consistency technique and record the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import os, re, ollama, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce042b9",
   "metadata": {},
   "source": [
    "## Define constants & utilities\n",
    "- Model: Self-hosted Llama 3.1 (8B parameters) from Ollama.\n",
    "- Dataset: [GSM8K](https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294&views%5B%5D=main_train). I used 100 samples of this dataset to make the evaluations run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e544f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "NUM_WORKERS = max(os.cpu_count() // 2, 1)\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", cache_dir=\"./data\")\n",
    "dataset = dataset[\"train\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbada31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, *, temperature: float) -> str:\n",
    "    try:\n",
    "        out = ollama.generate(\n",
    "            MODEL_NAME,\n",
    "            prompt,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"num_thread\": NUM_WORKERS\n",
    "            }\n",
    "        )\n",
    "        return out.response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "def extract_actual_answer(answer: str) -> str:\n",
    "    match = re.search(r'####\\s*([^\\n]+)', answer)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_llm_answer(text: str) -> str:\n",
    "    # Remove <think> sections\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Assume the final answer is after the last newline or after \"Answer:\"\n",
    "    lines = [line.strip() for line in text.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # Try to find a line starting with a digit\n",
    "    for line in reversed(lines):\n",
    "        if line.replace('.', '', 1).isdigit():\n",
    "            return line\n",
    "        if \"Answer\" in line:\n",
    "            parts = line.split()\n",
    "            for part in parts:\n",
    "                if part.replace('.', '', 1).isdigit():\n",
    "                    return part\n",
    "\n",
    "    return lines[-1]\n",
    "\n",
    "\n",
    "def evaluate_greedy_decoding() -> float:\n",
    "    correct = 0\n",
    "    total = len(dataset[\"question\"])\n",
    "\n",
    "    for i in tqdm(range(total), desc=\"Baseline\"):\n",
    "        question = dataset[\"question\"][i]\n",
    "        answer = extract_actual_answer(dataset[\"answer\"][i])\n",
    "\n",
    "        if answer is None:\n",
    "            print(f\"Skipping question {i} due to missing answer.\")\n",
    "            continue\n",
    "\n",
    "        prompt = f\"Question: {question}\\nLet's think step by step.\"\n",
    "        response = generate_response(prompt, temperature=0.0)\n",
    "        llm_answer = extract_llm_answer(response)\n",
    "\n",
    "        if llm_answer is None:\n",
    "            print(f\"Skipping question {i} due to missing LLM answer.\")\n",
    "            continue\n",
    "\n",
    "        if answer in llm_answer:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_self_consistency(samples: int) -> float:\n",
    "    correct = 0\n",
    "    total = len(dataset[\"question\"])\n",
    "\n",
    "    for i in tqdm(range(total), desc=f\"Self-Consistency (n={samples})\"):\n",
    "        question = dataset[\"question\"][i]\n",
    "        answer = extract_actual_answer(dataset[\"answer\"][i])\n",
    "\n",
    "        if answer is None:\n",
    "            print(f\"Skipping question {i} due to missing answer.\")\n",
    "            continue\n",
    "\n",
    "        answers = []\n",
    "        for _ in range(samples):\n",
    "            prompt = f\"Question: {question}\\nLet's think step by step.\"\n",
    "            response = generate_response(prompt, temperature=0.5)\n",
    "            llm_answer = extract_llm_answer(response)\n",
    "\n",
    "            if llm_answer is None:\n",
    "                print(f\"Skipping question {i} due to missing LLM answer.\")\n",
    "                continue\n",
    "\n",
    "            answers.append(llm_answer)\n",
    "\n",
    "        # Majority vote\n",
    "        most_common = Counter(answers).most_common(1)[0][0]\n",
    "        if answer in most_common:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def compute_accuracy(count: int) -> tuple[int, float]:\n",
    "    if count == 1:\n",
    "        acc = evaluate_greedy_decoding()\n",
    "    else:\n",
    "        acc = evaluate_self_consistency(samples=count)\n",
    "    print(f\"Samples: {count}, Accuracy: {acc:.2f}\")\n",
    "    return count, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0857c",
   "metadata": {},
   "source": [
    "## Run the Evaluation\n",
    "The below code evaluates the accuracy of the LLM on the dataset, using different sample counts in parallel.\n",
    "\n",
    "Using self-consistency, when a model is given a question, it generates multiple diverse outputs (i.e., reasoning paths) that each of them contain a final answer by solving the question in different ways. Among those outputs, the most common answer is chosen as the LLM's final answer to the given question.\n",
    "\n",
    "On the other hand, greedy decoding involves the model selecting the tokens with the highest probability that lead to one final answer, which is generally less accurate than self-consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts = [1, 3, 5, 10]\n",
    "accuracies = []\n",
    "\n",
    "# Parallel computation\n",
    "results = process_map(\n",
    "    compute_accuracy,\n",
    "    sample_counts,\n",
    "    max_workers=NUM_WORKERS,\n",
    "    desc=\"Evaluations\"\n",
    ")\n",
    "\n",
    "counts, accuracies = zip(*results)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(sample_counts, accuracies, marker='o')\n",
    "plt.title(\"Accuracy vs. Number of Samples\")\n",
    "plt.xlabel(\"Number of Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(sample_counts)\n",
    "plt.grid(True)\n",
    "plt.savefig('./graph.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
